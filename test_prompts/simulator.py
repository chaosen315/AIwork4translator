#!/usr/bin/env python3
"""
ä»¿çœŸæµ‹è¯•è„šæœ¬ - ç›´æ¥è°ƒç”¨moduleså·¥å…·æµ‹è¯•æç¤ºè¯æ•ˆæœ
æ¨¡æ‹ŸçœŸå®ç¿»è¯‘æµç¨‹ï¼Œç”¨äºéªŒè¯å’Œè°ƒä¼˜æç¤ºè¯
"""

import os
import sys
import json
import csv
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import time
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(dotenv_path="data/.env")

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from modules.api_tool import LLMService
from modules.csv_process_tool import load_terms_dict, find_matching_terms
from modules.read_tool import read_structured_paragraphs
from modules.config import global_config

class TranslationSimulator:
    """ç¿»è¯‘ä»¿çœŸå™¨ - æ¨¡æ‹ŸçœŸå®ç¿»è¯‘æµç¨‹"""
    
    def __init__(self, provider: str = "kimi"):
        self.llm_service = LLMService(provider=provider)
        self.provider = provider
        self.test_dir = Path(__file__).parent
        self.results_dir = self.test_dir / "simulation_results"
        self.results_dir.mkdir(exist_ok=True)
        
        # æµ‹è¯•æ–‡ä»¶è·¯å¾„
        self.samples_file = self.test_dir / "test_samples.md"
        self.terms_file = self.test_dir / "test_terms.csv"
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_segments = 0
        self.successful_segments = 0
        self.total_tokens = 0
        self.format_compliant_segments = 0
        
    def load_test_data(self) -> Tuple[List[Dict], Dict[str, str]]:
        """åŠ è½½æµ‹è¯•æ•°æ®å’Œæœ¯è¯­è¯å…¸"""
        print("ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®...")
        
        # åŠ è½½æ ·ä¾‹
        samples = []
        if self.samples_file.exists():
            with open(self.samples_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # æŒ‰æ ‡é¢˜åˆ†å‰²æ ·ä¾‹
            sections = content.split('\n## ')
            for section in sections[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ªæ ‡é¢˜
                lines = section.split('\n')
                title = lines[0].strip()
                text = '\n'.join(lines[1:]).strip()
                if text:
                    samples.append({
                        'title': title,
                        'text': text,
                        'id': len(samples) + 1
                    })
        
        # åŠ è½½æœ¯è¯­è¯å…¸
        terms_dict = {}
        if self.terms_file.exists():
            terms_dict = load_terms_dict(str(self.terms_file))
        
        print(f"âœ… åŠ è½½å®Œæˆï¼š{len(samples)} ä¸ªæ ·ä¾‹ï¼Œ{len(terms_dict)} ä¸ªæœ¯è¯­")
        return samples, terms_dict
    
    def simulate_translation_process(self, text: str, terms_dict: Dict[str, str], 
                                   sample_id: int, title: str) -> Dict:
        """æ¨¡æ‹Ÿå®Œæ•´çš„ç¿»è¯‘æµç¨‹"""
        print(f"\nğŸ”„ å¤„ç†æ ·ä¾‹ {sample_id}: {title}")
        print(f"åŸæ–‡é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # æ¨¡æ‹Ÿæ®µè½åˆ†å‰²ï¼ˆæŒ‰å®é™…æµç¨‹ï¼‰
        segments = self._split_into_segments(text)
        print(f"åˆ†å‰²ä¸º {len(segments)} ä¸ªæ®µè½")
        
        segment_results = []
        aggregated_new_terms = []
        
        for seg_idx, segment in enumerate(segments, 1):
            print(f"\nğŸ“„ ç¿»è¯‘æ®µè½ {seg_idx}/{len(segments)}")
            
            # æŸ¥æ‰¾åŒ¹é…çš„æœ¯è¯­ï¼ˆæ¨¡æ‹ŸçœŸå®æµç¨‹ï¼‰
            specific_terms = find_matching_terms(segment, terms_dict)
            if specific_terms:
                print(f"å‘ç° {len(specific_terms)} ä¸ªåŒ¹é…æœ¯è¯­")
            
            # åˆ›å»ºæç¤ºè¯ï¼ˆä½¿ç”¨å®é™…çš„create_promptæ–¹æ³•ï¼‰
            prompt = self.llm_service.create_prompt(segment, specific_terms)
            
            # è°ƒç”¨API
            try:
                response_obj, tokens = self.llm_service.call_ai_model_api(prompt)
                print(type(response_obj))
                print(response_obj)
                self.total_tokens += tokens
                self.successful_segments += 1

                translation = response_obj.get('translation', '')
                notes = response_obj.get('notes', '')
                print(type(notes))
                new_terms = response_obj.get('newterminology', [])
                aggregated_new_terms.extend(new_terms)
                joined = "\n\n---\n\n".join([translation, notes])

                print(f"âœ… ç¿»è¯‘æˆåŠŸ (tokens: {tokens})")

                # åˆ†æè¾“å‡ºæ ¼å¼
                format_analysis = self._analyze_output_format(joined)

                segment_results.append({
                    'segment_id': seg_idx,
                    'original': segment,
                    'translation_joined': joined,
                    'translation': translation,
                    'notes': notes,
                    'newterminology': new_terms,
                    'tokens': tokens,
                    'format_analysis': format_analysis,
                    'success': True
                })

                # æ˜¾ç¤ºç»“æœé¢„è§ˆ
                self._display_translation_preview(joined, format_analysis)
                
            except Exception as e:
                print(f"âŒ ç¿»è¯‘å¤±è´¥: {str(e)}")
                segment_results.append({
                    'segment_id': seg_idx,
                    'original': segment,
                    'translation': '',
                    'tokens': 0,
                    'format_analysis': {},
                    'success': False,
                    'error': str(e)
                })
            
            self.total_segments += 1
        
        return {
            'sample_id': sample_id,
            'title': title,
            'original_text': text,
            'segments': segment_results,
            'total_segments': len(segments),
            'successful_segments': sum(1 for seg in segment_results if seg['success']),
            'new_terms_total': len(aggregated_new_terms)
        }
    
    def _split_into_segments(self, text: str) -> List[str]:
        """æ¨¡æ‹Ÿæ®µè½åˆ†å‰²é€»è¾‘"""
        # ä½¿ç”¨ä¸çœŸå®æµç¨‹ç±»ä¼¼çš„é€»è¾‘
        max_size = global_config.max_chunk_size
        
        # é¦–å…ˆå°è¯•æŒ‰åŒæ¢è¡Œåˆ†å‰²
        paragraphs = text.split('\n\n')
        segments = []
        current_segment = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
                
            # å¦‚æœæ®µè½æœ¬èº«å°±å¾ˆé•¿ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
            if len(para) > max_size:
                # æŒ‰å¥å­åˆ†å‰²
                sentences = para.replace('. ', '.\n').replace('! ', '!\n').replace('? ', '?\n').split('\n')
                for sentence in sentences:
                    sentence = sentence.strip()
                    if len(sentence) > max_size:
                        # å¦‚æœå¥å­è¿˜å¤ªé•¿ï¼ŒæŒ‰å­—ç¬¦æ•°å¼ºåˆ¶åˆ†å‰²
                        for i in range(0, len(sentence), max_size):
                            chunk = sentence[i:i+max_size]
                            if chunk:
                                segments.append(chunk)
                    elif sentence:
                        segments.append(sentence)
            else:
                # æ£€æŸ¥æ˜¯å¦å¯ä»¥å°†å½“å‰æ®µè½æ·»åŠ åˆ°ç°æœ‰æ®µ
                if len(current_segment) + len(para) < max_size and current_segment:
                    current_segment += "\n\n" + para
                else:
                    if current_segment:
                        segments.append(current_segment)
                    current_segment = para
        
        if current_segment:
            segments.append(current_segment)
        
        return segments
    
    def _analyze_output_format(self, joined_text: str) -> Dict:
        analysis = {
            'has_main_text': False,
            'has_footnotes': False,
            'format_correct': False,
            'main_text': '',
            'footnotes': '',
            'issues': []
        }
        if not joined_text:
            analysis['issues'].append('ç¿»è¯‘ç»“æœä¸ºç©º')
            return analysis
        text = joined_text.replace('\r\n', '\n').strip()
        parts = None
        if '\n\n---\n\n' in text:
            parts = text.split('\n\n---\n\n', 1)
        elif '\n---\n' in text:
            parts = text.split('\n---\n', 1)
        else:
            lines = text.split('\n')
            sep_idx = -1
            for i, line in enumerate(lines):
                if line.strip() == '---':
                    sep_idx = i
                    break
            if sep_idx != -1:
                parts = ['\n'.join(lines[:sep_idx]), '\n'.join(lines[sep_idx+1:])]
        if parts:
            main_text = parts[0].strip()
            footnotes = parts[1].strip()
            analysis['main_text'] = main_text
            analysis['footnotes'] = footnotes
            analysis['has_main_text'] = bool(main_text)
            analysis['has_footnotes'] = bool(footnotes)
            bullet = any(l.strip().startswith('-') for l in footnotes.splitlines())
            analysis['format_correct'] = analysis['has_main_text'] and analysis['has_footnotes'] and bullet
            if not bullet:
                analysis['issues'].append('è¯‘æ³¨æœªä½¿ç”¨åˆ—è¡¨æ ¼å¼')
        else:
            analysis['main_text'] = text
            analysis['has_main_text'] = True
            analysis['issues'].append('æœªæ‰¾åˆ°åˆ†éš”ç¬¦ ---')
        return analysis
    
    def _display_translation_preview(self, translation: str, format_analysis: Dict):
        """æ˜¾ç¤ºç¿»è¯‘ç»“æœé¢„è§ˆ"""
        if format_analysis.get('format_correct', False):
            print("âœ… æ ¼å¼åˆè§„ - åŒæ®µå¼è¾“å‡º")
            main_text = format_analysis.get('main_text', '')
            footnotes = format_analysis.get('footnotes', '')
            print(f"æ­£æ–‡é¢„è§ˆ: {main_text[:100]}...")
            if footnotes:
                print(f"è¯‘æ³¨é¢„è§ˆ: {footnotes[:100]}...")
        else:
            print("âŒ æ ¼å¼ä¸åˆè§„")
            issues = format_analysis.get('issues', [])
            if issues:
                print(f"é—®é¢˜: {', '.join(issues)}")
            print(f"è¾“å‡ºé¢„è§ˆ: {translation[:150]}...")
    
    def run_simulation(self) -> None:
        """è¿è¡Œå®Œæ•´çš„ä»¿çœŸæµ‹è¯•"""
        print(f"\n{'='*80}")
        print(f"ğŸš€ å¼€å§‹ç¿»è¯‘ä»¿çœŸæµ‹è¯• - ä½¿ç”¨ {self.provider.upper()} æä¾›å•†")
        print(f"{'='*80}")
        
        start_time = time.time()
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        samples, terms_dict = self.load_test_data()
        
        if not samples:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•æ ·ä¾‹")
            return
        
        simulation_results = []
        
        # å¯¹æ¯ä¸ªæ ·ä¾‹è¿›è¡Œä»¿çœŸ
        for sample in samples:
            result = self.simulate_translation_process(
                sample['text'], terms_dict, sample['id'], sample['title']
            )
            simulation_results.append(result)
            
            # æ›´æ–°ç»Ÿè®¡
            for seg in result['segments']:
                if seg['success'] and seg['format_analysis'].get('format_correct', False):
                    self.format_compliant_segments += 1
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_simulation_report(simulation_results, start_time)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        self._save_simulation_results(simulation_results)
    
    def _generate_simulation_report(self, results: List[Dict], start_time: float) -> None:
        """ç”Ÿæˆä»¿çœŸæµ‹è¯•æŠ¥å‘Š"""
        print(f"\n{'='*80}")
        print("ğŸ“Š ä»¿çœŸæµ‹è¯•æŠ¥å‘Š")
        print(f"{'='*80}")
        
        total_samples = len(results)
        successful_samples = sum(1 for r in results if r['successful_segments'] > 0)
        
        print(f"æµ‹è¯•æ ·ä¾‹æ•°: {total_samples}")
        print(f"æˆåŠŸæ ·ä¾‹æ•°: {successful_samples}")
        print(f"æ€»æ®µè½æ•°: {self.total_segments}")
        print(f"æˆåŠŸç¿»è¯‘æ®µè½: {self.successful_segments}")
        print(f"æ ¼å¼åˆè§„æ®µè½: {self.format_compliant_segments}")
        print(f"æ€»tokensæ¶ˆè€—: {self.total_tokens}")
        
        if self.total_segments > 0:
            print(f"ç¿»è¯‘æˆåŠŸç‡: {self.successful_segments/self.total_segments*100:.1f}%")
            print(f"æ ¼å¼åˆè§„ç‡: {self.format_compliant_segments/self.total_segments*100:.1f}%")
        
        if self.successful_segments > 0:
            print(f"å¹³å‡æ¯æ®µtokens: {self.total_tokens/self.successful_segments:.0f}")
        
        print(f"æ€»è€—æ—¶: {time.time() - start_time:.2f}ç§’")
        
        # è¯¦ç»†åˆ†æ
        print(f"\n{'='*40}")
        print("è¯¦ç»†æ ·ä¾‹åˆ†æ:")
        print(f"{'='*40}")
        
        for result in results:
            print(f"\nğŸ“„ {result['title']}")
            print(f"  æ€»æ®µè½: {result['total_segments']}")
            print(f"  æˆåŠŸæ®µè½: {result['successful_segments']}")
            
            # ç»Ÿè®¡æ ¼å¼åˆè§„æƒ…å†µ
            compliant_segments = sum(
                1 for seg in result['segments'] 
                if seg['success'] and seg['format_analysis'].get('format_correct', False)
            )
            if result['successful_segments'] > 0:
                print(f"  æ ¼å¼åˆè§„: {compliant_segments}/{result['successful_segments']}")
    
    def _save_simulation_results(self, results: List[Dict]) -> None:
        """ä¿å­˜ä»¿çœŸç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜JSONæ ¼å¼
        json_file = self.results_dir / f"simulation_{self.provider}_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                'provider': self.provider,
                'timestamp': timestamp,
                'summary': {
                    'total_samples': len(results),
                    'total_segments': self.total_segments,
                    'successful_segments': self.successful_segments,
                    'format_compliant_segments': self.format_compliant_segments,
                    'total_tokens': self.total_tokens
                },
                'results': results
            }, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å¯è¯»æ ¼å¼
        txt_file = self.results_dir / f"simulation_{self.provider}_{timestamp}.txt"
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write(f"ç¿»è¯‘ä»¿çœŸæµ‹è¯•æŠ¥å‘Š\n")
            f.write(f"æä¾›å•†: {self.provider}\n")
            f.write(f"æ—¶é—´: {timestamp}\n")
            f.write(f"{'='*80}\n\n")
            
            for result in results:
                f.write(f"æ ·ä¾‹: {result['title']}\n")
                f.write(f"åŸæ–‡: {result['original_text'][:200]}...\n\n")
                
                for seg in result['segments']:
                    if seg['success']:
                        f.write(f"æ®µè½ {seg['segment_id']}:\n")
                        f.write(f"åŸæ–‡: {seg['original'][:100]}...\n")
                        f.write(f"è¯‘æ–‡: {seg['translation'][:100]}...\n")
                        
                        analysis = seg['format_analysis']
                        if analysis.get('format_correct', False):
                            f.write(f"æ ¼å¼: âœ… åˆè§„\n")
                            f.write(f"æ­£æ–‡: {analysis.get('main_text', '')[:100]}...\n")
                            f.write(f"è¯‘æ³¨: {analysis.get('footnotes', '')[:100]}...\n")
                        else:
                            f.write(f"æ ¼å¼: âŒ ä¸åˆè§„\n")
                            if analysis.get('issues'):
                                f.write(f"é—®é¢˜: {', '.join(analysis['issues'])}\n")
                        f.write("\n")
                f.write(f"{'='*40}\n\n")
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
        print(f"  JSONæ ¼å¼: {json_file}")
        print(f"  æ–‡æœ¬æ ¼å¼: {txt_file}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç¿»è¯‘æç¤ºè¯ä»¿çœŸæµ‹è¯•å·¥å…·')
    parser.add_argument('--provider', default='kimi', 
                       choices=['kimi', 'deepseek', 'gpt', 'silicon', 'gemini', 'doubao'],
                       help='é€‰æ‹©LLMæä¾›å•†')
    parser.add_argument('--sample', type=int, help='åªæµ‹è¯•æŒ‡å®šç¼–å·çš„æ ·ä¾‹')
    
    args = parser.parse_args()
    
    # åˆ›å»ºä»¿çœŸå™¨å¹¶è¿è¡Œ
    simulator = TranslationSimulator(provider=args.provider)
    
    if args.sample:
        # æµ‹è¯•æŒ‡å®šæ ·ä¾‹
        samples, terms_dict = simulator.load_test_data()
        if 0 < args.sample <= len(samples):
            sample = samples[args.sample - 1]
            result = simulator.simulate_translation_process(
                sample['text'], terms_dict, sample['id'], sample['title']
            )
            simulator._generate_simulation_report([result], time.time())
        else:
            print(f"âŒ æ ·ä¾‹ç¼–å·æ— æ•ˆï¼Œæœ‰æ•ˆèŒƒå›´: 1-{len(samples)}")
    else:
        # è¿è¡Œå®Œæ•´ä»¿çœŸ
        simulator.run_simulation()

if __name__ == "__main__":
    main()
